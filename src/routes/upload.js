/**
 * File upload route handlers.
 * Delegates storage operations to the configured storage adapter.
 * Handles multipart uploads via adapter logic.
 */

const express = require('express');
const router = express.Router();
const path = require('path'); // Still needed for extension checks
const { config } = require('../config');
const logger = require('../utils/logger');
const { storageAdapter } = require('../storage'); // Import the adapter factory's result
const { isDemoMode } = require('../utils/demoMode'); // Keep demo check for specific route behavior if needed

// --- Routes ---

// Initialize upload
router.post('/init', async (req, res) => {
  // Note: Demo mode might bypass storage adapter logic via middleware or adapter factory itself.
  // If specific demo responses are needed here, keep the check.
  if (isDemoMode()) {
    // Simplified Demo Response (assuming demoAdapter handles non-persistence)
     const { filename = 'demo_file', fileSize = 0 } = req.body;
     const demoUploadId = 'demo-' + Math.random().toString(36).substr(2, 9);
     logger.info(`[DEMO] Init request for ${filename}, size ${fileSize}. Returning ID ${demoUploadId}`);
     if (Number(fileSize) === 0) {
        logger.success(`[DEMO] Simulated completion of zero-byte file: ${filename}`);
        // Potentially call demoAdapter.completeUpload or similar mock logic if needed
     }
     return res.json({ uploadId: demoUploadId });
  }

  const { filename, fileSize } = req.body;
  const clientBatchId = req.headers['x-batch-id']; // Adapter might use this

  // --- Basic validations ---
  if (!filename) return res.status(400).json({ error: 'Missing filename' });
  if (fileSize === undefined || fileSize === null) return res.status(400).json({ error: 'Missing fileSize' });
  const size = Number(fileSize);
  if (isNaN(size) || size < 0) return res.status(400).json({ error: 'Invalid file size' });

  // --- Max File Size Check ---
  if (size > config.maxFileSize) {
    logger.warn(`Upload rejected: File size ${size} exceeds limit ${config.maxFileSize}`);
    return res.status(413).json({ error: 'File too large', limit: config.maxFileSize });
  }

  // --- Extension Check ---
  // Perform extension check before handing off to adapter
  if (config.allowedExtensions && config.allowedExtensions.length > 0) {
    const fileExt = path.extname(filename).toLowerCase();
    // Check if the extracted extension (including '.') is in the allowed list
    if (!fileExt || !config.allowedExtensions.includes(fileExt)) {
      logger.warn(`Upload rejected: File type not allowed: ${filename} (Extension: ${fileExt || 'none'})`);
      return res.status(400).json({ error: 'File type not allowed', receivedExtension: fileExt || 'none' });
    }
     logger.debug(`File extension ${fileExt} allowed for ${filename}`);
  }

  try {
    // Delegate initialization to the storage adapter
    const result = await storageAdapter.initUpload(filename, size, clientBatchId);

    // Respond with the uploadId generated by the adapter/system
    res.json({ uploadId: result.uploadId });

  } catch (err) {
    logger.error(`[Route /init] Upload initialization failed: ${err.message}`, err.stack);
    // Map common errors
    let statusCode = 500;
    let clientMessage = 'Failed to initialize upload.';
    if (err.message.includes('Invalid batch ID format')) {
        statusCode = 400;
        clientMessage = err.message;
    } else if (err.name === 'NoSuchBucket' || err.name === 'AccessDenied') { // S3 Specific
        statusCode = 500; // Internal config error
        clientMessage = 'Storage configuration error.';
    } else if (err.code === 'EACCES' || err.code === 'EPERM' || err.message.includes('writable')) { // Local Specific
         statusCode = 500;
         clientMessage = 'Storage permission or access error.';
    }
    // Add more specific error mapping based on adapter exceptions if needed

    res.status(statusCode).json({ error: clientMessage, details: err.message }); // Include details only for logging/debugging
  }
});

// Upload chunk
router.post('/chunk/:uploadId', express.raw({
  limit: config.maxFileSize + (10 * 1024 * 1024), // Allow slightly larger raw body than max file size
  type: 'application/octet-stream'
}), async (req, res) => {

  const { uploadId } = req.params;
  const chunk = req.body;
  const clientBatchId = req.headers['x-batch-id']; // May be useful for logging context

  // ** CRITICAL FOR S3: Get Part Number from client **
  // Client needs to send this, e.g., ?partNumber=1, ?partNumber=2, ...
  const partNumber = parseInt(req.query.partNumber || '1', 10);
   if (isNaN(partNumber) || partNumber < 1) {
      logger.error(`[Route /chunk] Invalid partNumber received: ${req.query.partNumber}`);
       return res.status(400).json({ error: 'Missing or invalid partNumber query parameter (must be >= 1)' });
   }

   // Demo mode handling (simplified)
   if (isDemoMode()) {
      logger.debug(`[DEMO /chunk] Received chunk for ${uploadId}, part ${partNumber}, size ${chunk?.length || 0}`);
      // Simulate progress - more sophisticated logic could go in a demoAdapter
      const demoProgress = Math.min(100, Math.random() * 100);
      const completed = demoProgress > 95; // Simulate completion occasionally
      if (completed) {
         logger.info(`[DEMO /chunk] Simulated completion for ${uploadId}`);
      }
      return res.json({ bytesReceived: 0, progress: demoProgress, completed }); // Approximate response
   }


  if (!chunk || chunk.length === 0) {
    logger.warn(`[Route /chunk] Received empty chunk for uploadId: ${uploadId}, part ${partNumber}`);
    return res.status(400).json({ error: 'Empty chunk received' });
  }


  try {
    // Delegate chunk storage to the adapter
    const result = await storageAdapter.storeChunk(uploadId, chunk, partNumber);

    // If the adapter indicates completion after storing this chunk, finalize the upload
    if (result.completed) {
      logger.info(`[Route /chunk] Chunk ${partNumber} for ${uploadId} triggered completion. Finalizing...`);
      try {
          const completionResult = await storageAdapter.completeUpload(uploadId);
          logger.success(`[Route /chunk] Successfully finalized upload ${uploadId}. Final path/key: ${completionResult.finalPath}`);
          // Send final success response (ensure progress is 100)
          return res.json({ bytesReceived: result.bytesReceived, progress: 100, completed: true });
      } catch (completionError) {
         logger.error(`[Route /chunk] CRITICAL: Failed to finalize completed upload ${uploadId} after storing chunk ${partNumber}: ${completionError.message}`, completionError.stack);
         // What to return to client? The chunk was stored, but completion failed.
         // Return 500, indicating server-side issue during finalization.
         return res.status(500).json({ error: 'Upload chunk received, but failed to finalize.', details: completionError.message });
      }
    } else {
      // Chunk stored, but upload not yet complete, return progress
      res.json({ bytesReceived: result.bytesReceived, progress: result.progress, completed: false });
    }

  } catch (err) {
    logger.error(`[Route /chunk] Chunk upload failed for ${uploadId}, part ${partNumber}: ${err.message}`, err.stack);
    // Map common errors
    let statusCode = 500;
    let clientMessage = 'Failed to process chunk.';

    if (err.message.includes('Upload session not found') || err.name === 'NoSuchUpload' || err.code === 'ENOENT') {
      statusCode = 404;
      clientMessage = 'Upload session not found or already completed/aborted.';
    } else if (err.name === 'InvalidPart' || err.name === 'InvalidPartOrder') { // S3 Specific
       statusCode = 400;
       clientMessage = 'Invalid upload chunk sequence or data.';
    } else if (err.name === 'SlowDown') { // S3 Throttling
       statusCode = 429;
       clientMessage = 'Upload rate limit exceeded by storage provider, please try again later.';
    } else if (err.code === 'EACCES' || err.code === 'EPERM' ) { // Local specific
        statusCode = 500;
        clientMessage = 'Storage permission error while writing chunk.';
    }
    // Add more specific error mapping if needed

    res.status(statusCode).json({ error: clientMessage, details: err.message });
  }
});

// Cancel upload
router.post('/cancel/:uploadId', async (req, res) => {
  const { uploadId } = req.params;

  if (isDemoMode()) {
      logger.info(`[DEMO /cancel] Request received for ${uploadId}`);
      // Call demoAdapter.abortUpload(uploadId) if it exists?
      return res.json({ message: 'Upload cancelled (Demo)' });
  }

  logger.info(`[Route /cancel] Received cancel request for upload: ${uploadId}`);

  try {
    // Delegate cancellation to the storage adapter
    await storageAdapter.abortUpload(uploadId);
    res.json({ message: 'Upload cancelled successfully or was already inactive.' });
  } catch (err) {
    // Abort errors are often less critical, log them but maybe return success anyway
    logger.error(`[Route /cancel] Error during upload cancellation for ${uploadId}: ${err.message}`, err.stack);
    // Don't necessarily send 500, as the goal is just to stop the upload client-side
    // Maybe just return success but log the server-side issue?
    // Or return 500 if S3 abort fails significantly? Let's return 500 for now.
    res.status(500).json({ error: 'Failed to cancel upload on server.', details: err.message });
  }
});

// Export the router, remove previous function exports
module.exports = { router };